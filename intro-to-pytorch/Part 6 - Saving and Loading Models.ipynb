{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and Loading Models\n",
    "\n",
    "In this notebook, I'll show you how to save and load models with PyTorch. This is important because you'll often want to load previously trained models to use in making predictions or to continue training on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import helper\n",
    "import fc_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "# Download and load the training data\n",
    "trainset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see one of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 389,
       "width": 389
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = next(iter(trainloader))\n",
    "helper.imshow(image[0,:]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a network\n",
    "\n",
    "To make things more concise here, I moved the model architecture and training code from the last part to a file called `fc_model`. Importing this, we can easily create a fully-connected network with `fc_model.Network`, and train the network using `fc_model.train`. I'll use this model (once it's trained) to demonstrate how we can save and load models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the network, define the criterion and optimizer\n",
    "\n",
    "model = fc_model.Network(784, 10, [512, 256, 128])\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2..  Training Loss: 1.678..  Test Loss: 0.994..  Test Accuracy: 0.638\n",
      "Epoch: 1/2..  Training Loss: 1.049..  Test Loss: 0.768..  Test Accuracy: 0.711\n",
      "Epoch: 1/2..  Training Loss: 0.898..  Test Loss: 0.682..  Test Accuracy: 0.748\n",
      "Epoch: 1/2..  Training Loss: 0.814..  Test Loss: 0.626..  Test Accuracy: 0.758\n",
      "Epoch: 1/2..  Training Loss: 0.706..  Test Loss: 0.614..  Test Accuracy: 0.770\n",
      "Epoch: 1/2..  Training Loss: 0.717..  Test Loss: 0.582..  Test Accuracy: 0.775\n",
      "Epoch: 1/2..  Training Loss: 0.695..  Test Loss: 0.591..  Test Accuracy: 0.770\n",
      "Epoch: 1/2..  Training Loss: 0.688..  Test Loss: 0.569..  Test Accuracy: 0.794\n",
      "Epoch: 1/2..  Training Loss: 0.627..  Test Loss: 0.541..  Test Accuracy: 0.795\n",
      "Epoch: 1/2..  Training Loss: 0.647..  Test Loss: 0.571..  Test Accuracy: 0.789\n",
      "Epoch: 1/2..  Training Loss: 0.644..  Test Loss: 0.564..  Test Accuracy: 0.794\n",
      "Epoch: 1/2..  Training Loss: 0.663..  Test Loss: 0.539..  Test Accuracy: 0.802\n",
      "Epoch: 1/2..  Training Loss: 0.645..  Test Loss: 0.526..  Test Accuracy: 0.813\n",
      "Epoch: 1/2..  Training Loss: 0.597..  Test Loss: 0.538..  Test Accuracy: 0.800\n",
      "Epoch: 1/2..  Training Loss: 0.628..  Test Loss: 0.516..  Test Accuracy: 0.808\n",
      "Epoch: 1/2..  Training Loss: 0.613..  Test Loss: 0.523..  Test Accuracy: 0.817\n",
      "Epoch: 1/2..  Training Loss: 0.575..  Test Loss: 0.517..  Test Accuracy: 0.813\n",
      "Epoch: 1/2..  Training Loss: 0.620..  Test Loss: 0.496..  Test Accuracy: 0.813\n",
      "Epoch: 1/2..  Training Loss: 0.571..  Test Loss: 0.510..  Test Accuracy: 0.816\n",
      "Epoch: 1/2..  Training Loss: 0.569..  Test Loss: 0.499..  Test Accuracy: 0.821\n",
      "Epoch: 1/2..  Training Loss: 0.585..  Test Loss: 0.478..  Test Accuracy: 0.829\n",
      "Epoch: 1/2..  Training Loss: 0.581..  Test Loss: 0.516..  Test Accuracy: 0.810\n",
      "Epoch: 1/2..  Training Loss: 0.545..  Test Loss: 0.476..  Test Accuracy: 0.822\n",
      "Epoch: 2/2..  Training Loss: 0.622..  Test Loss: 0.472..  Test Accuracy: 0.824\n",
      "Epoch: 2/2..  Training Loss: 0.564..  Test Loss: 0.472..  Test Accuracy: 0.824\n",
      "Epoch: 2/2..  Training Loss: 0.503..  Test Loss: 0.467..  Test Accuracy: 0.830\n",
      "Epoch: 2/2..  Training Loss: 0.532..  Test Loss: 0.468..  Test Accuracy: 0.828\n",
      "Epoch: 2/2..  Training Loss: 0.541..  Test Loss: 0.464..  Test Accuracy: 0.827\n",
      "Epoch: 2/2..  Training Loss: 0.565..  Test Loss: 0.467..  Test Accuracy: 0.824\n",
      "Epoch: 2/2..  Training Loss: 0.532..  Test Loss: 0.453..  Test Accuracy: 0.835\n",
      "Epoch: 2/2..  Training Loss: 0.519..  Test Loss: 0.461..  Test Accuracy: 0.830\n",
      "Epoch: 2/2..  Training Loss: 0.534..  Test Loss: 0.465..  Test Accuracy: 0.829\n",
      "Epoch: 2/2..  Training Loss: 0.510..  Test Loss: 0.457..  Test Accuracy: 0.836\n",
      "Epoch: 2/2..  Training Loss: 0.512..  Test Loss: 0.475..  Test Accuracy: 0.826\n",
      "Epoch: 2/2..  Training Loss: 0.532..  Test Loss: 0.452..  Test Accuracy: 0.836\n",
      "Epoch: 2/2..  Training Loss: 0.524..  Test Loss: 0.462..  Test Accuracy: 0.830\n",
      "Epoch: 2/2..  Training Loss: 0.522..  Test Loss: 0.448..  Test Accuracy: 0.834\n",
      "Epoch: 2/2..  Training Loss: 0.551..  Test Loss: 0.455..  Test Accuracy: 0.832\n",
      "Epoch: 2/2..  Training Loss: 0.538..  Test Loss: 0.445..  Test Accuracy: 0.838\n",
      "Epoch: 2/2..  Training Loss: 0.546..  Test Loss: 0.448..  Test Accuracy: 0.841\n",
      "Epoch: 2/2..  Training Loss: 0.497..  Test Loss: 0.437..  Test Accuracy: 0.843\n",
      "Epoch: 2/2..  Training Loss: 0.546..  Test Loss: 0.444..  Test Accuracy: 0.839\n",
      "Epoch: 2/2..  Training Loss: 0.532..  Test Loss: 0.453..  Test Accuracy: 0.834\n",
      "Epoch: 2/2..  Training Loss: 0.508..  Test Loss: 0.451..  Test Accuracy: 0.832\n",
      "Epoch: 2/2..  Training Loss: 0.518..  Test Loss: 0.437..  Test Accuracy: 0.842\n",
      "Epoch: 2/2..  Training Loss: 0.528..  Test Loss: 0.444..  Test Accuracy: 0.835\n"
     ]
    }
   ],
   "source": [
    "fc_model.train(model, trainloader, testloader, criterion, optimizer, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and loading networks\n",
    "\n",
    "As you can imagine, it's impractical to train a network every time you need to use it. Instead, we can save trained networks then load them later to train more or use them for predictions.\n",
    "\n",
    "The parameters for PyTorch networks are stored in a model's `state_dict`. We can see the state dict contains the weight and bias matrices for each of our layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our model: \n",
      "\n",
      " Network(\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  )\n",
      "  (output): Linear(in_features=128, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ") \n",
      "\n",
      "The state dict keys: \n",
      "\n",
      " odict_keys(['hidden_layers.0.weight', 'hidden_layers.0.bias', 'hidden_layers.1.weight', 'hidden_layers.1.bias', 'hidden_layers.2.weight', 'hidden_layers.2.bias', 'output.weight', 'output.bias'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Our model: \\n\\n\", model, '\\n')\n",
    "print(\"The state dict keys: \\n\\n\", model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest thing to do is simply save the state dict with `torch.save`. For example, we can save it to a file `'checkpoint.pth'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'saving_checkpoint_example.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can load the state dict with `torch.load`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['hidden_layers.0.weight', 'hidden_layers.0.bias', 'hidden_layers.1.weight', 'hidden_layers.1.bias', 'hidden_layers.2.weight', 'hidden_layers.2.bias', 'output.weight', 'output.bias'])\n"
     ]
    }
   ],
   "source": [
    "state_dict = torch.load('saving_checkpoint_example.pth')\n",
    "print(state_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to load the state dict in to the network, you do `model.load_state_dict(state_dict)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems pretty straightforward, but as usual it's a bit more complicated. Loading the state dict works only if the model architecture is exactly the same as the checkpoint architecture. If I create a model with a different architecture, this fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Network:\n\tsize mismatch for hidden_layers.0.weight: copying a param with shape torch.Size([512, 784]) from checkpoint, the shape in current model is torch.Size([400, 784]).\n\tsize mismatch for hidden_layers.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([400]).\n\tsize mismatch for hidden_layers.1.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([200, 400]).\n\tsize mismatch for hidden_layers.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([200]).\n\tsize mismatch for hidden_layers.2.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([100, 200]).\n\tsize mismatch for hidden_layers.2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([100]).\n\tsize mismatch for output.weight: copying a param with shape torch.Size([10, 128]) from checkpoint, the shape in current model is torch.Size([10, 100]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m fc_model\u001b[38;5;241m.\u001b[39mNetwork(\u001b[38;5;241m784\u001b[39m, \u001b[38;5;241m10\u001b[39m, [\u001b[38;5;241m400\u001b[39m, \u001b[38;5;241m200\u001b[39m, \u001b[38;5;241m100\u001b[39m])\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# This will throw an error because the tensor sizes are wrong!\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.14/lib/python3.8/site-packages/torch/nn/modules/module.py:1667\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1662\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[1;32m   1663\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1664\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[1;32m   1666\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 1667\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1668\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   1669\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Network:\n\tsize mismatch for hidden_layers.0.weight: copying a param with shape torch.Size([512, 784]) from checkpoint, the shape in current model is torch.Size([400, 784]).\n\tsize mismatch for hidden_layers.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([400]).\n\tsize mismatch for hidden_layers.1.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([200, 400]).\n\tsize mismatch for hidden_layers.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([200]).\n\tsize mismatch for hidden_layers.2.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([100, 200]).\n\tsize mismatch for hidden_layers.2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([100]).\n\tsize mismatch for output.weight: copying a param with shape torch.Size([10, 128]) from checkpoint, the shape in current model is torch.Size([10, 100])."
     ]
    }
   ],
   "source": [
    "# Try this\n",
    "model = fc_model.Network(784, 10, [400, 200, 100])\n",
    "# This will throw an error because the tensor sizes are wrong!\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means we need to rebuild the model exactly as it was when trained. Information about the model architecture needs to be saved in the checkpoint, along with the state dict. To do this, you build a dictionary with all the information you need to compeletely rebuild the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {'input_size': 784,\n",
    "              'output_size': 10,\n",
    "              'hidden_layers': [each.out_features for each in model.hidden_layers],\n",
    "              'state_dict': model.state_dict()}\n",
    "\n",
    "torch.save(checkpoint, 'saving_checkpoint_example_with_architecture.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_size': 784,\n",
       " 'output_size': 10,\n",
       " 'hidden_layers': [400, 200, 100],\n",
       " 'state_dict': OrderedDict([('hidden_layers.0.weight',\n",
       "               tensor([[-0.0119,  0.0069, -0.0159,  ..., -0.0241,  0.0034, -0.0168],\n",
       "                       [ 0.0105,  0.0354, -0.0071,  ...,  0.0113,  0.0227, -0.0149],\n",
       "                       [ 0.0156,  0.0049,  0.0237,  ...,  0.0269, -0.0117, -0.0183],\n",
       "                       ...,\n",
       "                       [-0.0143, -0.0138, -0.0143,  ...,  0.0245, -0.0013, -0.0055],\n",
       "                       [ 0.0146, -0.0312,  0.0272,  ..., -0.0252, -0.0063, -0.0336],\n",
       "                       [ 0.0281, -0.0042,  0.0133,  ...,  0.0005,  0.0301, -0.0039]])),\n",
       "              ('hidden_layers.0.bias',\n",
       "               tensor([-1.9350e-02,  3.4063e-02,  8.7757e-03, -1.6615e-02,  1.2852e-02,\n",
       "                        3.5445e-02, -2.5455e-02,  1.9955e-02, -5.1124e-03, -3.5399e-02,\n",
       "                        5.7671e-03,  6.0365e-03, -8.8321e-05, -4.9625e-03,  1.2178e-02,\n",
       "                       -2.6092e-02, -3.1286e-02,  2.1987e-03,  1.4550e-02,  6.2576e-03,\n",
       "                       -3.1552e-02,  3.2740e-02, -2.8332e-03, -1.8952e-02, -1.0559e-02,\n",
       "                       -2.4693e-02,  1.7549e-02,  2.2260e-02, -1.3904e-03, -8.9039e-03,\n",
       "                        4.7543e-03,  3.5286e-02, -1.5865e-03,  3.3723e-02,  9.7357e-03,\n",
       "                        3.4146e-02,  2.2290e-02,  1.9326e-02, -1.7795e-02, -2.5775e-03,\n",
       "                        3.6856e-03,  2.1445e-02,  1.0192e-02, -2.5640e-02,  2.9434e-02,\n",
       "                       -3.2856e-03, -1.3000e-02, -1.8412e-02, -1.7972e-02, -1.1098e-02,\n",
       "                        2.2785e-02,  2.2224e-02,  3.0008e-02,  2.0733e-02,  9.0995e-05,\n",
       "                        1.7790e-02,  1.9436e-02,  3.1885e-02, -3.1822e-02, -1.3927e-02,\n",
       "                        1.5360e-02,  1.2584e-02, -2.9897e-02, -1.2984e-02, -8.2914e-03,\n",
       "                       -8.1189e-03,  7.1330e-03,  4.2538e-03, -3.0000e-02,  9.5351e-03,\n",
       "                       -2.8818e-02, -3.3288e-02, -3.0291e-02, -1.3332e-03, -8.7641e-04,\n",
       "                       -2.4302e-02,  2.1508e-02,  4.6817e-03, -2.4445e-02,  2.5698e-02,\n",
       "                       -3.2288e-02,  1.2363e-02, -3.2340e-03, -2.8440e-02,  2.3610e-02,\n",
       "                       -1.2442e-02, -2.5021e-02, -2.9308e-02,  2.4539e-03,  1.3452e-03,\n",
       "                       -2.2526e-02,  3.0472e-02, -2.4356e-02,  3.2623e-02, -2.1752e-03,\n",
       "                        3.4571e-02,  2.2998e-02,  6.6751e-03, -9.1660e-03,  2.8466e-02,\n",
       "                        6.3646e-03,  2.6054e-02,  9.4300e-04, -1.5762e-03,  9.5984e-04,\n",
       "                       -2.8508e-02, -2.0443e-02, -1.4876e-02, -1.7842e-02, -4.5716e-03,\n",
       "                        3.1862e-02, -2.1685e-02,  1.4516e-02,  2.8199e-02, -3.5208e-02,\n",
       "                       -2.8345e-02,  3.2309e-02,  2.4042e-02, -2.7617e-02,  3.4146e-02,\n",
       "                       -1.4850e-02,  2.8534e-03, -1.3560e-02,  1.8135e-03, -2.4205e-02,\n",
       "                        1.4252e-02, -3.1842e-02,  1.5513e-02,  2.4209e-02, -2.2878e-02,\n",
       "                        1.2296e-02, -1.0185e-02, -3.3010e-02,  2.1691e-02, -2.0621e-02,\n",
       "                       -1.5032e-02,  1.0373e-02, -3.4193e-02,  1.4375e-02,  1.2923e-02,\n",
       "                        1.0293e-02, -1.8593e-03, -3.5236e-02,  3.4820e-02,  1.7716e-02,\n",
       "                        7.3972e-03, -1.5723e-02,  2.3359e-02,  1.6777e-02, -2.1324e-02,\n",
       "                       -2.4446e-02,  3.8971e-03,  4.8853e-03,  3.3305e-02, -2.8388e-02,\n",
       "                        7.8718e-03, -2.7465e-02,  2.3873e-02, -2.5031e-02,  1.0742e-02,\n",
       "                        2.7036e-02, -8.1905e-04,  1.8783e-02,  1.8475e-02,  1.4821e-02,\n",
       "                       -1.3275e-02,  5.5316e-03, -2.9242e-02, -2.3904e-03,  2.9949e-02,\n",
       "                        3.0219e-02, -1.0977e-02,  3.4387e-02,  8.6528e-03, -3.3231e-02,\n",
       "                        8.6183e-03, -1.7866e-02,  1.8968e-02,  1.4320e-02, -2.9833e-02,\n",
       "                       -2.1001e-02,  2.2968e-02, -2.2858e-03,  2.3869e-02,  3.3777e-02,\n",
       "                        1.9537e-02,  2.6251e-02, -2.1127e-02, -2.5051e-02,  2.0938e-02,\n",
       "                        2.2046e-02, -1.1258e-02,  2.7094e-02, -6.9483e-03,  2.1668e-02,\n",
       "                       -2.1987e-02, -3.2973e-02, -4.7965e-03,  2.2243e-02,  2.7733e-03,\n",
       "                        5.0484e-03,  1.3358e-02,  2.8624e-03,  3.4773e-02, -2.8362e-02,\n",
       "                        2.4997e-02,  1.0223e-02,  2.4431e-02, -1.2391e-02, -1.1059e-02,\n",
       "                       -3.1893e-02, -9.6348e-03, -2.9699e-02,  3.3764e-02, -1.8588e-02,\n",
       "                        3.0856e-02,  1.1838e-02,  3.3634e-02,  5.0481e-03, -1.0110e-02,\n",
       "                       -2.1874e-02, -9.6550e-03,  1.8657e-02,  2.0927e-03,  1.7363e-02,\n",
       "                        2.6905e-02, -1.9991e-03,  3.3455e-02, -1.3800e-02,  9.3347e-03,\n",
       "                       -2.3264e-02, -1.8862e-02,  1.1107e-02, -5.0889e-04, -3.4507e-02,\n",
       "                       -1.2162e-02, -2.2758e-02, -2.8566e-03, -3.9884e-03,  3.1655e-02,\n",
       "                       -7.0579e-03, -9.7130e-03,  3.3595e-02,  1.9212e-02,  1.6968e-02,\n",
       "                       -1.1041e-02,  2.1175e-02, -2.3036e-03, -1.6061e-03,  1.8536e-02,\n",
       "                       -9.6693e-03, -2.1318e-03,  1.0031e-02, -7.3056e-03,  2.9840e-02,\n",
       "                       -1.3849e-02,  1.9217e-04,  3.1604e-02,  2.2273e-02,  1.8316e-02,\n",
       "                        2.4674e-02,  2.1805e-02,  3.2399e-02, -9.7681e-04,  7.1256e-03,\n",
       "                       -3.8188e-03, -1.4099e-02,  6.4083e-03, -2.2336e-02,  1.5774e-02,\n",
       "                       -6.6728e-03, -1.4208e-02,  4.1208e-03, -3.3647e-02, -1.4736e-03,\n",
       "                        3.0840e-02, -4.1415e-03, -4.0624e-03, -1.1146e-02,  1.0514e-02,\n",
       "                        2.7352e-02, -1.1929e-02, -3.1288e-02, -1.1440e-02, -1.1837e-02,\n",
       "                       -2.7473e-02,  3.3414e-02, -8.9232e-03,  1.8032e-02,  3.2937e-02,\n",
       "                        3.3985e-03,  3.5163e-02, -4.3415e-03, -2.4726e-02,  2.5369e-02,\n",
       "                       -1.1936e-02, -2.8869e-02,  2.3559e-02,  1.3872e-02, -3.0776e-02,\n",
       "                       -8.1364e-03, -2.8379e-02,  3.5333e-03,  1.5510e-02,  1.7890e-03,\n",
       "                        5.2207e-03, -2.1599e-02, -9.3178e-03, -2.3474e-02,  1.1630e-02,\n",
       "                       -5.1727e-03, -4.2660e-03, -1.7463e-02, -8.1018e-03, -7.4135e-03,\n",
       "                       -2.3657e-02,  2.4244e-02, -1.9875e-02,  3.4267e-02, -8.3493e-03,\n",
       "                        3.1129e-02, -2.5983e-02,  1.9234e-02,  1.3086e-02, -1.7804e-03,\n",
       "                       -2.4919e-02,  3.4038e-02, -1.1568e-02,  2.0860e-02, -1.9576e-02,\n",
       "                        1.4910e-02, -1.6824e-02, -7.0818e-03, -2.7346e-02,  3.1520e-02,\n",
       "                       -3.5225e-02,  1.4209e-02, -1.9536e-02, -1.6845e-02, -1.3266e-03,\n",
       "                       -3.4339e-02, -1.5108e-02, -2.3283e-02,  1.4553e-02, -1.5182e-02,\n",
       "                        3.0773e-02,  3.1917e-03, -1.9131e-02, -3.1662e-02,  6.8206e-03,\n",
       "                       -1.8320e-02, -1.8433e-02,  1.9991e-02,  1.8621e-02, -8.8124e-03,\n",
       "                       -7.7706e-03, -8.7593e-04, -1.3433e-02, -3.2149e-02, -3.3350e-02,\n",
       "                        1.7274e-03, -1.6647e-02, -2.1422e-02, -3.7855e-04,  3.2930e-02,\n",
       "                       -2.1986e-02, -1.5590e-03,  2.8253e-02, -1.2811e-02,  1.8207e-02,\n",
       "                        7.4560e-03,  2.7929e-02,  3.2778e-03,  2.2700e-02, -3.1005e-02,\n",
       "                        2.8012e-02,  3.0199e-03, -2.9244e-02, -1.9133e-02,  2.6083e-02,\n",
       "                       -2.2690e-02, -2.2074e-02,  1.5951e-02, -2.9652e-02,  2.3685e-02,\n",
       "                       -2.8981e-02, -1.8988e-02, -3.3602e-02,  1.0545e-02,  3.7829e-04,\n",
       "                       -1.9109e-02, -5.7467e-03, -2.0063e-02,  2.6525e-02, -3.8848e-03,\n",
       "                       -1.1634e-02,  3.3521e-02, -2.3552e-02, -1.1467e-02, -3.2630e-02])),\n",
       "              ('hidden_layers.1.weight',\n",
       "               tensor([[-0.0196,  0.0306,  0.0002,  ..., -0.0403, -0.0456,  0.0169],\n",
       "                       [ 0.0396,  0.0285, -0.0070,  ...,  0.0497, -0.0131,  0.0251],\n",
       "                       [ 0.0085,  0.0184,  0.0311,  ...,  0.0105,  0.0135, -0.0229],\n",
       "                       ...,\n",
       "                       [ 0.0388,  0.0087,  0.0226,  ...,  0.0278,  0.0422,  0.0463],\n",
       "                       [ 0.0098, -0.0373,  0.0302,  ..., -0.0403, -0.0343, -0.0126],\n",
       "                       [-0.0089, -0.0226,  0.0028,  ..., -0.0176,  0.0281,  0.0266]])),\n",
       "              ('hidden_layers.1.bias',\n",
       "               tensor([ 4.5564e-02, -3.4691e-02,  1.4812e-02,  4.2503e-02,  4.2481e-02,\n",
       "                        1.3421e-02,  4.0306e-02,  1.4155e-02,  2.3656e-02, -4.7793e-02,\n",
       "                       -2.7169e-02, -7.5861e-03,  1.7583e-02,  1.2380e-02, -1.6518e-02,\n",
       "                        3.1465e-03,  3.8818e-02,  4.9269e-03,  2.6895e-02,  4.0816e-02,\n",
       "                       -4.4119e-02,  4.7202e-02,  2.9563e-02,  2.9087e-02,  7.2126e-03,\n",
       "                       -9.8262e-03, -3.4032e-02,  2.2051e-02,  3.2757e-02, -4.1541e-02,\n",
       "                        3.6809e-03,  8.0249e-03,  4.3612e-02, -4.3756e-02,  3.3384e-03,\n",
       "                        4.2904e-02, -2.8289e-02, -2.5360e-02, -2.1776e-02,  2.5595e-02,\n",
       "                       -4.8304e-02, -3.3392e-02,  1.0801e-02, -1.4347e-02, -3.4917e-02,\n",
       "                        1.4060e-02,  2.1271e-02, -4.8811e-02, -1.1805e-02,  4.0344e-02,\n",
       "                        3.2001e-02,  4.0639e-02,  7.2563e-05, -1.8121e-03,  1.7343e-03,\n",
       "                        3.0242e-03,  2.0748e-03, -3.0165e-04, -3.2757e-02,  4.9778e-02,\n",
       "                        2.6111e-02, -3.4229e-02,  3.9197e-02, -2.7448e-03,  9.5972e-03,\n",
       "                        3.6015e-02,  6.7317e-03, -2.8825e-02,  3.9297e-03,  2.7255e-02,\n",
       "                       -3.8043e-02,  2.8887e-02,  4.1262e-02, -5.3469e-03, -1.0642e-02,\n",
       "                        9.0400e-03, -2.6248e-02,  2.5910e-02,  1.6531e-02, -2.3585e-02,\n",
       "                        3.1106e-02, -7.9401e-03,  3.0150e-02, -2.4426e-02,  4.3986e-02,\n",
       "                       -3.4567e-02, -1.3590e-02, -2.6564e-03,  2.8033e-02,  3.8569e-02,\n",
       "                        7.5911e-03,  4.0655e-02,  3.3594e-02, -4.1208e-02, -3.6017e-02,\n",
       "                       -3.7972e-02,  1.3752e-02,  4.4407e-02,  3.8245e-02, -3.9278e-02,\n",
       "                        3.3077e-02, -2.0155e-02, -3.6492e-02,  1.2919e-02,  1.9311e-02,\n",
       "                        4.1368e-02,  4.4684e-02,  3.8510e-02,  1.2445e-02,  2.4981e-02,\n",
       "                       -4.3854e-02, -1.7146e-02,  3.5376e-02,  4.7023e-02, -3.0505e-02,\n",
       "                       -3.2572e-02,  4.8381e-03, -5.9161e-03, -1.4628e-02,  3.4932e-02,\n",
       "                       -4.7203e-03, -9.6264e-04,  7.2934e-03,  1.5511e-02,  3.2035e-02,\n",
       "                       -2.5981e-02,  3.8058e-02, -4.2351e-02,  1.2099e-02,  1.4515e-02,\n",
       "                       -3.8668e-03,  3.9330e-02,  4.6838e-03, -3.3965e-02,  4.3685e-03,\n",
       "                       -2.7657e-02,  1.8087e-02,  3.0297e-02,  4.7609e-02, -2.5452e-02,\n",
       "                       -5.8349e-03, -1.8607e-02,  1.3674e-02, -1.6021e-02, -1.4678e-02,\n",
       "                        6.5928e-03, -1.1783e-02,  3.1453e-02,  2.8270e-02,  3.6899e-02,\n",
       "                       -7.5049e-03,  4.8753e-02, -1.6864e-02, -2.4045e-02,  5.3905e-03,\n",
       "                       -2.1733e-02, -2.4471e-02, -3.7267e-02,  3.9797e-03,  6.0078e-03,\n",
       "                       -1.0184e-02,  4.7194e-02,  6.2448e-05, -2.5492e-02,  3.5485e-02,\n",
       "                       -3.4747e-02,  4.3244e-02, -8.7517e-03,  2.2403e-02,  4.3440e-02,\n",
       "                       -2.3649e-02, -4.1635e-02,  3.0726e-02,  1.7318e-02, -2.0130e-02,\n",
       "                        3.4837e-03, -3.8732e-02,  4.0343e-02,  1.2800e-02, -4.0464e-02,\n",
       "                        9.3354e-03, -2.2385e-02,  1.7755e-02,  2.2548e-02,  1.1191e-02,\n",
       "                       -1.7267e-03, -1.7198e-03,  2.5589e-02, -1.3293e-02, -4.8986e-02,\n",
       "                        3.8387e-02, -4.7243e-02, -3.3940e-02, -4.7322e-02, -2.7307e-02,\n",
       "                       -4.4214e-02,  3.0000e-02, -1.4295e-02, -4.3966e-02,  4.5960e-02])),\n",
       "              ('hidden_layers.2.weight',\n",
       "               tensor([[ 0.0230,  0.0215,  0.0547,  ...,  0.0225, -0.0126,  0.0283],\n",
       "                       [ 0.0346, -0.0094, -0.0677,  ..., -0.0546,  0.0507,  0.0496],\n",
       "                       [ 0.0608,  0.0211,  0.0428,  ...,  0.0095,  0.0165,  0.0688],\n",
       "                       ...,\n",
       "                       [ 0.0633, -0.0397, -0.0361,  ..., -0.0502,  0.0184,  0.0233],\n",
       "                       [-0.0259, -0.0580, -0.0250,  ...,  0.0127, -0.0223,  0.0601],\n",
       "                       [ 0.0597,  0.0402, -0.0400,  ...,  0.0277, -0.0269, -0.0103]])),\n",
       "              ('hidden_layers.2.bias',\n",
       "               tensor([ 0.0685,  0.0092, -0.0022, -0.0653, -0.0139,  0.0452, -0.0356,  0.0399,\n",
       "                        0.0155, -0.0399, -0.0032, -0.0165,  0.0390, -0.0213,  0.0020,  0.0558,\n",
       "                        0.0061, -0.0619, -0.0386, -0.0366,  0.0204,  0.0226, -0.0470,  0.0551,\n",
       "                        0.0119, -0.0454,  0.0477,  0.0512,  0.0055, -0.0501,  0.0626,  0.0266,\n",
       "                        0.0388, -0.0605,  0.0591, -0.0015,  0.0477,  0.0419, -0.0112, -0.0599,\n",
       "                       -0.0302,  0.0085, -0.0122,  0.0061,  0.0516, -0.0236,  0.0581,  0.0252,\n",
       "                       -0.0177, -0.0237, -0.0441,  0.0637,  0.0500, -0.0547, -0.0699, -0.0499,\n",
       "                       -0.0172,  0.0671,  0.0297, -0.0413,  0.0297,  0.0674,  0.0447,  0.0303,\n",
       "                       -0.0397, -0.0091, -0.0211, -0.0118, -0.0448, -0.0570, -0.0597, -0.0045,\n",
       "                       -0.0009,  0.0379, -0.0400, -0.0430, -0.0427, -0.0495, -0.0430, -0.0104,\n",
       "                        0.0470, -0.0069,  0.0194,  0.0269, -0.0170, -0.0291,  0.0667, -0.0472,\n",
       "                       -0.0172, -0.0057,  0.0597, -0.0623,  0.0634, -0.0170,  0.0004,  0.0141,\n",
       "                        0.0231,  0.0591,  0.0338,  0.0117])),\n",
       "              ('output.weight',\n",
       "               tensor([[ 1.0612e-02,  7.5623e-02,  2.7366e-02,  5.0984e-02, -7.4312e-02,\n",
       "                        -3.6372e-02, -2.4965e-02,  3.7775e-02, -1.1312e-02, -2.5731e-02,\n",
       "                        -4.5478e-03,  2.1255e-02, -5.9403e-03, -7.5500e-02,  2.7393e-02,\n",
       "                        -1.7850e-02,  7.4291e-02,  3.1864e-02,  7.2738e-02,  5.0780e-02,\n",
       "                        -8.6473e-02,  8.4277e-02,  2.8026e-02,  1.2458e-02, -5.0988e-02,\n",
       "                         2.1266e-02,  4.4825e-02, -7.7174e-03, -1.1563e-06, -7.4338e-02,\n",
       "                        -2.5977e-02,  6.7457e-02,  9.2406e-02,  2.7895e-02,  5.2963e-02,\n",
       "                         8.0115e-02, -9.8917e-02, -3.2709e-02,  9.5215e-02, -4.6094e-02,\n",
       "                         5.6135e-02, -9.8163e-02,  2.9331e-02, -3.7750e-02, -8.4776e-02,\n",
       "                        -7.1445e-02, -4.5249e-02,  9.9617e-02, -5.4056e-02, -4.2929e-02,\n",
       "                        -5.0834e-02, -7.5066e-02, -3.8760e-02,  3.2677e-02,  1.8147e-02,\n",
       "                        -4.3397e-02, -7.9063e-02, -4.1762e-02, -8.0365e-02,  7.5501e-02,\n",
       "                         2.0378e-02, -9.5444e-02, -4.7553e-04,  6.0160e-02, -1.1427e-02,\n",
       "                         2.8093e-02, -8.8321e-02,  9.9388e-02,  4.6563e-02, -2.7978e-02,\n",
       "                         8.8987e-02,  5.0941e-02, -8.2371e-02,  9.8964e-02,  7.3315e-02,\n",
       "                         6.6970e-03, -6.1471e-02, -9.1552e-02, -5.2528e-02,  1.5579e-02,\n",
       "                        -3.5230e-02,  4.0747e-02, -2.5176e-02, -1.6798e-02, -5.4132e-03,\n",
       "                        -2.2959e-02,  8.2062e-02, -2.1484e-02,  6.8910e-02,  4.8896e-02,\n",
       "                         3.4754e-02,  4.0380e-02, -4.6309e-02,  5.2175e-02,  9.9787e-02,\n",
       "                         4.4318e-02,  6.7466e-02,  9.6556e-02,  4.0483e-02,  6.6151e-02],\n",
       "                       [-1.4782e-04,  6.9315e-02, -2.3935e-03, -8.5646e-02,  2.2666e-02,\n",
       "                        -7.1079e-02,  5.3761e-02, -8.2940e-02,  7.2335e-02,  6.3799e-02,\n",
       "                        -4.2784e-02, -1.7793e-02,  7.8692e-03, -9.9194e-03,  2.8010e-02,\n",
       "                         5.1821e-02, -5.1637e-02,  2.3078e-02,  8.8703e-02, -5.8731e-02,\n",
       "                        -3.2324e-02, -3.5690e-02, -2.1651e-02, -4.4045e-02, -9.3646e-02,\n",
       "                         7.8460e-02,  7.2331e-02, -1.3612e-02,  8.7995e-02,  8.5662e-02,\n",
       "                        -6.0395e-02, -8.1738e-02,  6.2809e-02, -6.8785e-02, -2.1318e-02,\n",
       "                        -6.4878e-02,  2.9344e-02,  3.2487e-02,  1.2052e-02, -7.0483e-02,\n",
       "                         5.2406e-02,  9.9010e-02, -8.0674e-02,  9.7863e-02,  1.7101e-02,\n",
       "                        -1.9891e-02, -7.9971e-02,  7.9397e-02,  9.1518e-02,  2.4900e-02,\n",
       "                        -4.2926e-02, -7.2673e-02, -9.5632e-02,  7.9427e-02, -4.5021e-02,\n",
       "                        -8.1899e-02,  6.0070e-02, -7.7845e-03,  9.1100e-02,  9.9940e-02,\n",
       "                         9.2164e-02,  5.7295e-02,  3.9950e-02, -7.7120e-02,  7.2114e-02,\n",
       "                        -5.6826e-02, -2.5353e-02, -9.9604e-02,  5.7079e-02,  3.7054e-02,\n",
       "                        -2.2510e-02,  3.3342e-03, -7.0940e-02,  9.2490e-03,  5.1704e-02,\n",
       "                        -5.2286e-02,  4.8091e-02,  1.1966e-02, -4.7633e-02, -8.9967e-02,\n",
       "                         3.3788e-03, -9.5959e-02, -4.3789e-02,  8.1545e-02, -3.0289e-03,\n",
       "                        -7.1536e-02, -2.8401e-02, -3.6444e-02,  9.3940e-02, -7.7303e-02,\n",
       "                        -2.6454e-02, -7.4540e-02,  5.2427e-02,  8.5343e-02, -8.4081e-02,\n",
       "                         6.2495e-02, -5.3369e-02, -4.1126e-02, -5.3595e-02, -5.7371e-02],\n",
       "                       [ 7.0687e-02,  6.5960e-02, -2.9793e-02, -7.7907e-02, -7.6510e-02,\n",
       "                         5.1706e-02,  9.2456e-02, -8.9358e-02,  3.4349e-02,  7.8782e-02,\n",
       "                        -4.4833e-02,  8.3949e-03,  2.4179e-02,  7.9594e-02, -8.6638e-02,\n",
       "                         7.9257e-02, -9.9604e-02, -2.6239e-02,  2.5067e-02, -7.9701e-02,\n",
       "                         2.2653e-02,  7.8068e-02,  2.3983e-02,  8.9539e-02, -8.4893e-02,\n",
       "                        -8.3681e-03, -1.1862e-02,  6.3666e-02, -8.9582e-02, -3.7487e-02,\n",
       "                        -4.5124e-02, -2.0148e-02,  5.8385e-02, -2.1793e-02, -7.6074e-02,\n",
       "                         3.3049e-02,  9.6906e-02,  3.5266e-02, -1.4257e-02, -6.1421e-02,\n",
       "                         4.1331e-02, -3.4821e-02, -6.0550e-02,  6.5818e-02,  1.9676e-02,\n",
       "                         8.7888e-02, -6.8389e-03, -6.6345e-03,  9.8691e-02,  5.3743e-03,\n",
       "                        -9.1410e-02,  7.8279e-02,  8.5196e-02,  5.3609e-02,  9.2888e-02,\n",
       "                         9.7569e-02,  5.9859e-02, -5.4589e-02,  8.3930e-02, -7.5293e-02,\n",
       "                        -4.6131e-03, -3.0641e-02,  8.1530e-02, -9.9507e-02,  3.6974e-02,\n",
       "                        -5.7225e-02, -2.2255e-02,  4.4323e-02, -2.8576e-02,  3.8585e-02,\n",
       "                         8.8908e-02,  4.3273e-02, -2.1663e-02,  3.4472e-02, -2.0560e-02,\n",
       "                         4.2927e-02,  5.2488e-02,  5.7729e-02,  4.4168e-03, -6.6042e-02,\n",
       "                        -5.2040e-02, -7.6188e-02,  2.1469e-02,  8.9555e-02, -3.9618e-02,\n",
       "                        -4.9105e-02, -5.0572e-02,  6.6650e-03, -8.8550e-02,  6.6406e-02,\n",
       "                        -6.9399e-02,  1.5179e-02, -3.4566e-03, -5.1948e-02,  6.4006e-02,\n",
       "                        -2.0795e-03,  3.2308e-02, -7.2180e-02,  4.2912e-02,  7.5828e-02],\n",
       "                       [ 6.6754e-03,  2.2723e-02, -3.1597e-02, -7.9572e-02,  6.8342e-02,\n",
       "                         5.7701e-02,  5.7177e-02,  1.8862e-02, -7.0062e-02,  8.9797e-02,\n",
       "                        -8.4931e-02,  8.8396e-02,  4.8886e-02, -2.9013e-02,  6.6857e-03,\n",
       "                        -7.6697e-02,  7.6935e-02, -2.0530e-02, -2.3887e-03, -6.8401e-02,\n",
       "                         4.9592e-02,  6.5209e-02,  3.7986e-02,  2.4964e-02, -8.4842e-02,\n",
       "                        -6.2646e-02, -9.4695e-02,  7.3065e-02,  9.9214e-04,  8.3098e-02,\n",
       "                        -3.5303e-03, -6.0144e-02,  5.4118e-02, -5.7354e-02,  2.7086e-02,\n",
       "                        -8.8830e-02,  9.1792e-02, -4.2646e-02,  8.5615e-03, -7.6674e-02,\n",
       "                        -1.9414e-03, -2.4266e-02,  6.8674e-02,  3.5831e-02,  9.5619e-02,\n",
       "                        -2.6756e-02,  2.0267e-02,  5.0572e-02,  9.5198e-02,  2.3951e-02,\n",
       "                        -6.3343e-02,  4.4599e-02, -9.8696e-02,  8.7159e-03, -1.2757e-03,\n",
       "                        -8.2862e-02, -7.3200e-02,  7.2088e-02,  4.8353e-02, -2.2021e-02,\n",
       "                         8.6223e-02, -7.9463e-03, -8.7550e-02,  1.0416e-02, -4.5458e-02,\n",
       "                        -3.9547e-02,  3.3411e-02, -6.4914e-03, -8.5565e-02, -7.4433e-02,\n",
       "                         9.9722e-02,  4.8019e-02, -4.9726e-02, -9.1437e-02, -1.8517e-02,\n",
       "                         3.9695e-02,  7.5395e-02, -1.0365e-02,  2.1872e-02, -5.1195e-02,\n",
       "                         2.1859e-02,  8.5681e-03,  6.2243e-02,  3.3891e-02, -6.2118e-02,\n",
       "                         1.3301e-02, -3.6445e-02, -7.5257e-02, -2.6897e-02,  3.8910e-02,\n",
       "                         8.3678e-03, -5.9129e-02,  8.1215e-02, -1.4149e-03,  8.5290e-02,\n",
       "                         2.3152e-02, -9.7850e-02, -2.3280e-02,  7.4480e-02, -7.7044e-02],\n",
       "                       [ 9.6977e-03, -4.2072e-02,  8.4921e-02,  5.7244e-02,  2.9430e-03,\n",
       "                        -7.8156e-02,  1.2760e-02, -2.2968e-02, -3.4429e-02, -7.1855e-03,\n",
       "                         2.5813e-02, -4.7933e-02, -5.4914e-02, -4.4519e-02,  6.9177e-02,\n",
       "                        -5.3314e-02,  3.5786e-02,  2.2289e-02, -1.5199e-05,  5.7876e-02,\n",
       "                        -4.1029e-02, -6.7923e-02, -4.1138e-02,  5.7882e-02,  3.8651e-02,\n",
       "                         5.1872e-02,  5.7534e-02, -8.5275e-02,  1.3601e-02, -2.1410e-02,\n",
       "                         8.7694e-03,  9.8734e-02, -7.7887e-03, -4.0990e-02, -3.6722e-02,\n",
       "                         9.2802e-02, -5.8878e-02, -3.1185e-02, -9.1478e-02,  3.7630e-02,\n",
       "                         5.9013e-02,  9.5892e-02, -5.1456e-02, -8.2524e-02,  8.3793e-02,\n",
       "                         3.2541e-02, -4.4100e-02, -9.4656e-02, -5.1263e-02,  3.0241e-02,\n",
       "                        -6.4069e-02,  1.5350e-02, -6.0642e-02,  9.5661e-02,  5.7207e-02,\n",
       "                         6.6860e-02, -9.2672e-02,  5.7760e-02,  4.5056e-02,  4.8873e-02,\n",
       "                         3.8263e-03,  1.8467e-02,  2.6912e-02, -6.3521e-02, -1.9170e-02,\n",
       "                         7.2253e-02,  1.8856e-02, -2.2729e-02, -7.8050e-02,  7.3942e-02,\n",
       "                        -3.6753e-02,  9.8184e-02, -3.1609e-02,  1.0384e-02,  1.6813e-02,\n",
       "                        -1.1516e-03,  2.2131e-02, -4.9501e-02,  8.7878e-02, -5.0662e-02,\n",
       "                         6.6009e-02, -7.0142e-02,  5.3827e-02,  4.2854e-02, -2.4829e-02,\n",
       "                         8.3965e-02, -1.6178e-02,  1.3911e-02,  4.1484e-02, -8.0582e-02,\n",
       "                         4.4968e-02, -5.4929e-03, -4.8380e-02,  9.7948e-03,  5.1234e-02,\n",
       "                         9.8779e-02, -7.2203e-02,  4.6139e-02, -8.5275e-02, -4.9254e-02],\n",
       "                       [ 4.4014e-02, -5.6735e-02,  1.5208e-02, -1.3754e-02,  3.0923e-02,\n",
       "                         3.2531e-02,  3.1561e-02, -3.6831e-02, -8.4198e-02, -5.2483e-02,\n",
       "                         2.5579e-02, -1.9097e-02, -9.6369e-02,  2.5458e-02, -1.0384e-02,\n",
       "                        -5.8263e-02,  6.2468e-02, -6.3868e-02,  6.4344e-02,  8.6108e-02,\n",
       "                        -8.1016e-02,  8.9156e-02,  8.1277e-02,  8.6486e-02, -1.5377e-02,\n",
       "                         2.7101e-02,  4.5623e-02, -2.7968e-02,  1.7300e-02, -5.0431e-02,\n",
       "                        -1.9657e-02, -5.2989e-02,  7.8401e-02,  5.1496e-02,  1.6465e-02,\n",
       "                        -1.0351e-02, -5.7666e-02,  5.8327e-03,  1.0710e-02,  7.1354e-02,\n",
       "                         6.8835e-02, -7.5494e-02, -1.5287e-02, -9.4607e-02, -2.6988e-02,\n",
       "                         4.2541e-02, -3.6688e-03, -6.7904e-02,  2.9053e-02,  2.4078e-02,\n",
       "                        -9.2567e-02, -8.3014e-02,  3.9384e-02, -7.0940e-02, -8.8724e-02,\n",
       "                         6.9276e-02, -1.7101e-03, -5.5872e-02, -4.1030e-02, -1.1081e-03,\n",
       "                         5.5371e-02, -3.3280e-02, -7.7449e-02, -1.1948e-02, -3.7923e-02,\n",
       "                        -2.8739e-02,  2.8788e-02, -5.8119e-02, -8.7704e-02, -1.6545e-02,\n",
       "                         5.6082e-02,  6.2313e-02, -2.6053e-02,  3.3805e-02, -7.7265e-02,\n",
       "                        -7.8345e-02,  7.2836e-02, -8.0019e-02,  2.0450e-02,  4.6461e-02,\n",
       "                         3.6891e-02,  7.5600e-02,  6.3252e-02, -1.2942e-02,  8.2990e-02,\n",
       "                         6.0094e-03, -3.1606e-02,  1.9204e-02,  5.0131e-02,  5.4727e-02,\n",
       "                        -6.9690e-02,  8.0325e-02, -2.8011e-02, -1.7918e-02, -2.7522e-02,\n",
       "                        -8.9615e-02,  9.7722e-03, -2.9489e-02, -9.7219e-02,  2.1426e-03],\n",
       "                       [ 7.3038e-02,  8.7666e-02, -8.0089e-02,  7.3135e-02, -5.1656e-02,\n",
       "                         8.7673e-02, -9.5403e-02, -3.3444e-02, -2.5994e-02, -6.2116e-04,\n",
       "                        -6.8928e-02,  8.3436e-02,  5.7510e-02,  6.4251e-02,  9.6538e-02,\n",
       "                         4.3810e-02, -4.4157e-02, -8.3673e-02, -8.6275e-02,  3.2488e-02,\n",
       "                         7.9813e-02, -4.7516e-02,  8.6702e-02, -2.8657e-02,  5.2336e-02,\n",
       "                        -3.6986e-02, -6.0783e-02, -1.1137e-02, -3.1973e-02,  9.0880e-02,\n",
       "                         1.3928e-02,  1.5517e-02, -7.0726e-02,  1.6506e-03,  6.3435e-02,\n",
       "                        -5.2472e-02,  7.3294e-02, -6.5893e-02, -8.4649e-02, -7.7753e-02,\n",
       "                         3.3740e-02, -1.7448e-02,  7.6194e-02,  1.1735e-02,  4.7479e-02,\n",
       "                         4.2164e-02, -6.1899e-02,  8.5037e-02, -7.3144e-02, -5.3688e-02,\n",
       "                        -7.3045e-02,  1.4997e-02,  3.5761e-02, -7.3289e-02, -1.4585e-02,\n",
       "                         7.7348e-02, -8.0912e-02,  4.8684e-02,  1.2804e-02,  9.0178e-04,\n",
       "                        -9.2756e-02, -3.9519e-02,  8.2388e-02,  6.9292e-02,  9.6768e-03,\n",
       "                         9.4721e-02,  3.3534e-02, -1.6672e-02,  1.0151e-02,  5.6260e-02,\n",
       "                         2.2451e-02, -1.4453e-02, -6.3813e-02, -2.2282e-02, -2.6986e-02,\n",
       "                         9.3597e-02,  4.1702e-02,  6.9546e-03,  5.6688e-02,  6.5604e-02,\n",
       "                        -2.5363e-02,  4.3494e-03, -4.5804e-02,  8.9943e-02,  1.6356e-02,\n",
       "                        -7.5322e-02, -5.1242e-02, -6.6828e-02, -5.4796e-02,  7.4816e-02,\n",
       "                         6.3160e-02, -8.7169e-02,  4.6679e-03,  1.7789e-02,  6.4988e-02,\n",
       "                         1.2240e-02, -8.7688e-03,  9.5117e-02,  1.4412e-02,  7.7841e-02],\n",
       "                       [-5.2122e-02, -1.4846e-02,  1.1139e-02, -4.2020e-02, -8.8869e-02,\n",
       "                         5.7794e-02, -6.3151e-02, -6.1736e-02, -1.4120e-02,  5.5917e-02,\n",
       "                         5.2897e-02,  3.6190e-02, -3.9014e-02, -2.3957e-02,  9.3032e-02,\n",
       "                         3.6467e-03,  6.6798e-02, -5.5591e-02, -4.5487e-02,  5.9419e-02,\n",
       "                        -9.9852e-03,  9.5543e-02, -8.6318e-02, -3.5285e-02, -7.2484e-02,\n",
       "                        -4.8316e-02, -7.5597e-03,  1.0899e-02, -3.4203e-02, -1.2921e-02,\n",
       "                        -7.2992e-02, -4.5011e-03,  1.4924e-03, -4.9084e-02,  3.1649e-02,\n",
       "                         7.2742e-02,  3.8184e-02, -1.1289e-02,  3.3296e-02,  4.1550e-02,\n",
       "                        -1.3242e-02, -1.9755e-02,  9.6364e-02, -9.6728e-02,  2.1798e-02,\n",
       "                         1.9174e-02, -8.7540e-02,  2.0800e-02,  2.8079e-02, -5.8711e-02,\n",
       "                        -1.5055e-02,  4.8344e-02, -5.4554e-02,  2.6809e-02,  2.3505e-02,\n",
       "                         3.7165e-02,  1.5225e-02,  6.5897e-02,  4.7353e-02,  1.9011e-03,\n",
       "                        -2.8494e-02, -3.4823e-02,  2.4035e-02, -9.9767e-02,  6.4988e-02,\n",
       "                        -3.9226e-02,  2.2680e-02, -4.4532e-02, -4.0198e-02,  8.6313e-02,\n",
       "                         8.2331e-03, -6.9410e-02, -2.8334e-02,  7.4302e-02, -4.0888e-02,\n",
       "                        -7.5160e-04, -4.0113e-02,  2.2476e-02, -2.4772e-02, -3.8188e-02,\n",
       "                        -2.1357e-02,  7.4406e-02, -5.8281e-02, -2.8356e-02,  2.7762e-02,\n",
       "                         7.6570e-02, -6.7635e-02,  7.3890e-02, -4.1859e-02, -2.2621e-02,\n",
       "                         2.7102e-02, -2.1216e-02,  9.5253e-02, -1.4554e-02,  8.4704e-02,\n",
       "                        -2.8728e-03, -4.3670e-02,  6.5748e-02,  8.0843e-02,  6.9288e-02],\n",
       "                       [-4.5323e-03, -6.3864e-02,  3.6813e-02,  4.6758e-02, -6.8547e-02,\n",
       "                         5.2953e-02,  7.3972e-02,  4.1889e-02,  3.6558e-02,  5.9408e-02,\n",
       "                        -6.0982e-02, -1.8053e-02, -7.0958e-02,  4.0349e-02, -9.6174e-02,\n",
       "                        -6.0142e-02, -8.2233e-02,  2.4345e-02,  6.6093e-02,  6.1313e-02,\n",
       "                        -1.4355e-02,  7.8530e-02, -1.0746e-02,  9.7040e-02,  7.6888e-03,\n",
       "                        -9.1070e-02,  6.0174e-02, -9.2735e-02, -1.2512e-02, -7.9102e-02,\n",
       "                        -1.7724e-02,  7.7550e-02,  7.2785e-02,  5.6038e-03,  8.7061e-02,\n",
       "                         1.5775e-02, -6.7211e-02,  8.2977e-02, -7.7974e-02,  1.6582e-02,\n",
       "                         5.3867e-02, -1.6470e-02, -1.1554e-03, -2.1429e-02, -8.9346e-03,\n",
       "                        -1.8223e-02,  5.1653e-02, -9.5149e-02, -4.3743e-02,  8.7526e-02,\n",
       "                        -6.9928e-02,  7.6990e-03,  3.6204e-02,  8.0412e-02, -9.1856e-02,\n",
       "                         1.6647e-02, -6.8673e-02,  9.0573e-02,  1.9700e-02, -6.0547e-02,\n",
       "                         3.9215e-02, -1.5599e-02, -1.3173e-02, -6.9938e-02,  8.0759e-02,\n",
       "                        -2.0037e-02, -2.7759e-02,  3.2076e-02, -8.7016e-02,  2.0908e-02,\n",
       "                        -7.3575e-02, -9.1605e-02, -5.8193e-02, -3.9288e-02,  3.4000e-02,\n",
       "                         7.0845e-02, -3.8492e-02,  3.1396e-02, -6.5585e-02,  1.5270e-02,\n",
       "                        -6.3428e-02, -1.8892e-02, -4.1187e-03, -4.6095e-02, -4.6576e-02,\n",
       "                         9.5179e-02, -8.2554e-02, -7.5882e-03,  6.5730e-02, -2.0254e-02,\n",
       "                        -8.2052e-02, -8.5364e-02,  3.5329e-02,  4.7616e-02,  8.3330e-02,\n",
       "                         7.6844e-02,  7.2636e-02, -6.5785e-02, -6.5510e-02,  8.3760e-02],\n",
       "                       [-6.8446e-02, -5.9302e-02,  2.2453e-02, -2.3515e-02, -3.4487e-02,\n",
       "                         7.9560e-02, -6.3934e-02,  2.9458e-02,  9.9907e-02,  5.2035e-02,\n",
       "                        -4.3386e-02,  7.4912e-02,  6.3930e-02,  6.2105e-02,  8.3675e-02,\n",
       "                         4.5715e-02,  9.4306e-02,  1.8453e-02,  1.2365e-02,  8.8921e-04,\n",
       "                         9.0343e-02,  8.1912e-02,  1.1807e-02, -7.9976e-02,  4.1036e-02,\n",
       "                         1.5513e-02,  7.8123e-02, -2.7945e-02, -3.8334e-02, -7.0019e-02,\n",
       "                         3.5757e-02,  1.4084e-02, -9.6570e-02,  2.5649e-02,  4.5936e-02,\n",
       "                         2.9894e-02,  5.0370e-02,  6.4298e-02, -9.6667e-02,  5.6605e-02,\n",
       "                        -3.1876e-02, -4.6714e-02,  7.2680e-02,  5.2800e-02, -4.4963e-02,\n",
       "                         9.2430e-02,  8.5933e-02,  4.0142e-02, -8.6731e-02,  9.9393e-02,\n",
       "                        -5.8003e-02,  2.9304e-02, -9.5906e-02, -9.4013e-02, -8.0911e-02,\n",
       "                        -5.6533e-02, -9.8764e-02, -9.4319e-02,  3.5291e-02,  4.5079e-02,\n",
       "                         5.2483e-02, -2.9525e-02, -3.7535e-02, -7.6336e-02,  4.8861e-02,\n",
       "                         3.9354e-02, -1.7303e-02, -1.8724e-02, -2.4670e-02, -9.0593e-02,\n",
       "                         3.7603e-02, -1.9385e-02,  5.8940e-02,  4.9746e-03, -5.9443e-02,\n",
       "                         3.5138e-02,  7.4130e-02,  5.5627e-03, -8.3810e-03, -5.5479e-02,\n",
       "                        -8.0658e-02, -8.5643e-02,  5.9716e-02,  2.5314e-02,  4.3003e-02,\n",
       "                         3.7867e-02,  3.6487e-02, -1.1392e-02, -2.6628e-03, -9.2325e-02,\n",
       "                        -9.9912e-02,  1.2618e-02, -2.4666e-02,  1.0840e-02,  8.7225e-02,\n",
       "                         3.6754e-02, -1.4504e-02,  5.5617e-03,  2.6469e-02, -1.4215e-02]])),\n",
       "              ('output.bias',\n",
       "               tensor([-0.0671, -0.1598,  0.1288,  0.1066,  0.0098,  0.0538, -0.0016,  0.0301,\n",
       "                        0.0010, -0.0312]))])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the checkpoint has all the necessary information to rebuild the trained model. You can easily make that a function if you want. Similarly, we can write a function to load checkpoints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(filepath):\n",
    "    checkpoint = torch.load(filepath)\n",
    "    model = fc_model.Network(checkpoint['input_size'],\n",
    "                             checkpoint['output_size'],\n",
    "                             checkpoint['hidden_layers'])\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network(\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): Linear(in_features=784, out_features=400, bias=True)\n",
      "    (1): Linear(in_features=400, out_features=200, bias=True)\n",
      "    (2): Linear(in_features=200, out_features=100, bias=True)\n",
      "  )\n",
      "  (output): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = load_checkpoint('saving_checkpoint_example_with_architecture.pth')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.14 64-bit ('3.8.14')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "e0c21b23c047c136636d9646f6dd22481417ea9d5d1b49f44ba332ea4add255e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
